# CSE 247 Notes

Washington University in St. Louis

Created for ‘Data Structures & Algorithms’ (CSE 247) 

Nathaniel Hayman

## Summations

*Exam 1*

Some useful summation formulas are:

$$
\sum_{i=0}^{n-1}i=\frac{n(n+1)}{2}
$$

$$
\sum_{i=0}^{n-1}i^2=\frac{n(n+1)(2n+1)}{6}
$$

$$
\sum_{i=0}^{n}\log i=\log(n)!
$$

$$
\sum_{i=0}^{n-1}a^i=\frac{1-a^n}{1-a}
$$

Note that if the bounds of the sum do not conform to those listed above, a change of base becomes necessary:

$$
\sum_{i=a}^{b}f(i)=\sum_{i=0}^{b-a}f(i-a)
$$

## Runtime Complexity

*Exam 1*

### *General Rules*

$$
f(n)=O(g(n))\iff c\cdot g(n)\geq f(n) 
$$

$$
f(n)=\Omega(g(n))\iff c\cdot g(n)\leq f(n) 
$$

$$
\small\text{for some }c>0,n>n_0
$$

$$
f(n)=\Theta(g(n))\iff f(n)=O(g(n))\land f(n)=\Omega(g(n))
$$

### *Runtimes of Different Algorithms*

Search algorithms:

- *Binary Search*: $O(n)$ time complexity, $O(1)$ auxiliary space complexity
- *Linear Search*: $O(n)$ time complexity, $O(1)$ auxiliary space complexity

Sort algorithms:

- *Merge Sort*: $O(n\log n)$ time complexity, $O(n)$ auxiliary space complexity
- *Bubble Sort* $O(n^2)$ time complexity, $O(1)$ auxiliary space complexity
- *Insertion Sort* $O(n^2)$ time complexity, $O(1)$ auxiliary space complexity

*Quick Sort*

- $O(n^2)$ time complexity, $O(1)$ auxiliary space complexity;

| ADT | Insert | Remove | Find | Max/Min | Sort | Instantiate |
| --- | --- | --- | --- | --- | --- | --- |
| Heap | $\Theta(\log n)$ | $\Theta(\log n)$ | $\Theta(n)$ | $\Theta(1)$ | $\Theta(\log n)$ | $\Theta(\log n)$ |
| BST | $\Theta(n)$ | $\Theta(n)$ | $\Theta(n)$ | $\Theta(n)$ |  | $\Theta(\log n)$ |
| AVL | $\Theta(\log n)$ | $\Theta(\log n)$ | $\Theta(\log n)$ | $\Theta(\log n)$ |  | $\Theta(\log n)$ |
| B-Tree | $\Theta(\log n)$ | $\Theta(\log n)$ | $\Theta(\log n)$ | $\Theta(\log n)$ |  | $\Theta(\log n)$ |
| LL | $\Theta(n)$ | $\Theta(n)$ | $\Theta(n)$ | $\Theta(n)$ |  | $\Theta(n)$ |
| Queue | $\Theta(1)$ | $\Theta(1)$ |  |  |  | $\Theta(n)$ |
| Stack | $\Theta(1)$ | $\Theta(1)$ |  |  |  | $\Theta(n)$ |

## Master Theorem

*Exam 2*

### *Recurrences*

The time complexity of a recurrence can be defined as the following:

$$
T(n)=aT(n/b)+f(n)\\
T(1)=d\\
\space\\
T(n)=dn^{\log_ab}+f(n)+\sum_{j=1}^{\log_bn-1}a^jf(n/b^j)
$$

### ********The Theorem********

- **************Case 1**************: $f(n)=O(n^{\log_ba-\epsilon})$ for some $\epsilon > 0$
    
    $\Longrightarrow T(n)=\Theta(n^{log_ba})$
    
- **************Case 2**************: $f(n)=O(n^{\log_ba}\log^kn)$ for some $k\in\Z$
    - ****************Case 2a****************: $k>-1$
        
        $\Longrightarrow T(n)=\Theta(n^{log_ba}\log^{k+1}n)$
        
    - ************Case 2b************: $k=-1$
        
        $\Longrightarrow T(n)=\Theta(n^{log_ba}\log\log n)$
        
    - ********Case 2b********: $k<-1$
        
        $\Longrightarrow T(n)=\Theta(n^{log_ba})$
        
- **************Case 3**************: $f(n)=\Omega(n^{log_ba+\epsilon})$ for some $\epsilon>0$
    
    $\Longrightarrow T(n)=\Theta(f(n)) \iff af(\frac{n}{b})\leq kf(n)$ for some $k<1$ 
    

### *Basic Requirements*

If a given time complexity does not follow the following rules, the Master Method cannot be used on it: 

- $a$ is a constant and $a\geq1$
- The difference between $f(n)$  and $n\log_ba$  is polynomial
    - If this not the case and $T(n)=2T(n/2)+\frac{n}{\log n}$, $T(n)=(n\log\log n)$.
- $f(n)$ is positive

## Sorting Algorithms

*Exam 2*

### *Background*

- A single comparison runs in $O(1)$ time
- Any fixed algorithm will give an **upper bound** on the cost of the fastest possible, for sorting algorithms this means $O(n\log n)$
- We can say that $\Omega(n)$ is the **lower bound** of sorting because each element must be visited *at least once*

### *Decision Trees & Comparison Sort*

Visualize process and number of outputs of a sorting algorithm:

![Untitled](CSE%20247%20Notes%2014c56a9a63644ee6a5c8752f8b3aae2a/Untitled.png)

We can describe this tree using $w$, the number of possible outcomes (two in the above example), as well as the number of leaf nodes $t(n)$.

Combining the two, the *minimum* number of operations required to solve the tree is $\log_wt(n)$.

Since comparison sort has two possible outcomes ($\leq$ and $>$), $w=2$. Since leaves of the decision tree are all potential organizations of the original array, $t(n)=n!$

Therefore, comparison sort is $O(n\log n)$ and $\Omega(n\log n)$ by limit comparison of $\log_wt(n)=\log_2n!$ with $n\log n$.

Thus, the limitation of comparison sorts is that, by definition, it employs *comparisons* and can only operate in at best $O(n\log n)$ time as a result.

### *Counting Sort*

Counting sort is an alternative to comparison sorting that is superior for sorting an array of integers or keys because it takes advantage of the fact that integers, within a finite range, can be counted and mapped simply by iterating through the array.

The basic algorithm is as follows:

- Tally (count) the number of instances of each integer in the array
- Iterate over each element of the range and generate an output array with sequential batches of numbers based upon the original tally

This makes counting sort’s runtime $\Theta(n+k)$, where $k$ is the size of the input range, a complexity that boils down to $\Theta(n)$ since $k$ is considered to be a constant.

### *Radix Sort*

Radix sort utilizes counting sort to sort an array of integers by least-significant digits, improving upon the efficiency of counting sort for sufficiently large sets of integers since counting sort would have to iterate over each element of the input range while radix sort does not.

Radix sort is *stable*, meaning that it does not invert the order of two inputs with the same key between passes.

Runtime complexity of $O(n)$.

Here is an example of Radix Sort being used on an input array $I_0$ of size $7$, with a matrix representation of the internal buckets given by $B_i$ for array $I_i$, the alerted input array at step $i$  of the algorithm (note that $|B_i|=|I_0|\times m$, for $\exists m\leq|I_0|$):

$$
I_0=[142, 632, 542, 279, 350, 715, 347]
$$

$$
B_0=\begin{bmatrix}
     350 \\ 
     \space \\
     142 & 632 & 542\\
     \space \\
     \space \\
     715 \\
     \space \\
     347 \\
     \space \\
     297 \\
 \end{bmatrix}
$$

$$
I_1=[350, 142, 632, 542, 715, 347, 279]
$$

$$
B_1=\begin{bmatrix}
     \space \\ 
     142\space \\
     297\\
     347 & 350\space \\
     \space \\
     542 \\
     631 \space \\
     715 \\
     \space \\
     \space \\
 \end{bmatrix}
$$

$$
I_2=[715, 631, 142, 542, 347, 350, 279]
$$

$$
B_2=\begin{bmatrix}
     \space \\ 
     142\space \\
     297\\
     347 & 350\space \\
     \space \\
     542 \\
     631 \space \\
     715 \\
     \space \\
     \space \\
 \end{bmatrix}
$$

$$
I_f=[142, 279, 347, 350, 542, 631, 715]
$$

## Hash Tables

*Exam 2*

### *Dictionaries*

Defined as a set of key-value pairs, such that a key can be utilized to efficiently target a value stored in a collection.

Methods include:

- Insert to add a record to the dictionary
- Find to locate a record (or records) based on key
- Remove a record (or records) by key

### *Hash Table Implementation*

The hash function for a hash table takes in the key of a record to be stored in the table, and outputs a hash code as an integer in a defined range $[0,N]$. The record is then mapped to a bucket in the hash table using index mapping. If there is already one or more records present in said bucket, the records are chained (usually via a linked list) inside of the bucket.

### *Efficacy*

All hash functions *must* return the same hash code for equivalent inputs.

Beyond that basic rule, a good hash function will *uniformly distribute* inputs between the buckets of the hash table.

Hashing these hash codes into buckets is also a point of contention, but there are two straightforward options:

- Division Hashing: $b(c)=c\mod m$ where $b(c)$ represents the index of the bucket to be used, $c$ the hash code, and $m$ the table size. Unfortunately, division hashing encounters problems when the hash code output range generates values with similar factors (the output range is often not uniform)
- Multiplicative Hashing: $b(c)=⌊((cA)\mod 1.0)m⌋$ where $c,m$ are defined above and $A$ is some constant.
    - To make this form of hashing most effective, $A$ should not be overly small, but should have a large amount of repeating decimals ($\frac{\pi}{4}$ is a great value for $A$)

The *load factor* of a hash table is $\alpha=\frac{n}{m}$ for $n$ entries and $m$ buckets.

A constant load factor $L$ is an upper limit on $\alpha$ such that $\alpha\leq L$.

This can be maintained by expanding the number of buckets and reallocating when $\alpha>L$.

## Binary Search Tree

*Exam 2*

### *BST Properties*

A BST fulfills the basic property that for any node in the tree $x$, $x\leq r$ and $x\geq l$ where $r$ is the right child node of $x$ and $l$ is the left child node.

### *Finding the Successor*

To find the successor node of a node in a BST, we follow the following steps:

- If the node has a right subtree: find the *leftmost* (minimum) node in the tree
- If the node has no right subtree, follow the targeted node’s parent pointers upwards until one of those parents is a right parent.

### *Finding the Predecessor*

Finding a processor in a BST is almost the same as finding the successor, you simply have to switch any left operations to right operations and vice versa.

To find the predecessor of a node in a BST we follow the following steps:

- If the node has a left subtree: find the *rightmost* (minimum) node in the tree
- If the node has no left subtree, follow the targeted node’s parent pointers upwards until one of those parents is a left parent.

### *Removing a Node*

To remove a node from a BST, simply replace it with its successor

## AVL Tree

*Exam 2*

An AVL Tree is a type of BST. What happens if you input a sorted array into a BST? You get a BST with height and it only goes one direction. The more compact the tree, the more efficient, so the AVL is a way to make sure the tree is compact (balanced).

Each node has a balance factor $B$ that can be calculated by taking the difference between the left and right child (some people do left-right, some do right-left, either work but there will be a few differences in the code).

### *Computing Height*

The general equation for the height of a node (which can actually be described as a subtree) is

$$
h=m(\{h_l,h_r\})+1
$$

Where the function $m(X)$ represents the maximum value of the input set $X$, and $h_l$ and $h_r$ represent the heights of the left and right subtrees, respectively.

Obviously a recursive definition such as this requires a base case, which would apply to the leaf nodes for a BST. The height of a leaf node is defined to be 0.

For an AVL tree to be balanced, for every node $i$, $-2\leq B_i\leq 2$. If this property is violated, the tree must be rebalanced

### *Rebalancing*

An AVL rotation is employed when the balance factor needs to be restored to the original range of $-1\leq B\leq 1$.

The algorithm behind the rebalance method can be described as follows:

- **Case 1:** $B=2$, implying that the left subtree is taller than the right since $B=h_l-h_r$.

*Result*: If $B_l=-1$, perform a leftward rotation, then perform a rightward rotation regardless

- **Case 2: $B=-2$**, implying that the right subtree is taller than the left.

*Result*: If $B_r=1$, perform a rightward rotation, then perform a leftward rotation regardless A rotation always changes the root of the subtree(this subtree could be the entire tree as well).

### *Right & Left Rotations*

The basic pseudocode for a rightward rotation is as follows:

```python
rotate_right(r):	
	r0 = r.left.right
	if r.parent:
		r.left.parent = None
		r.left.right = r
		r = r.left
		r.right.left = r0
	
	update_height(r.right)
	update_height(r)
```

A leftward rotation follows a very similar algorithm, but will all the instances of “right” replaced with “left” and vice versa:

```python
rotate_left(r):	
	l0 = r.right.left
	if r.parent:
		r.right.parent = None
		r.right.left = r
		r = r.right
			r.right.left = l0
	
	update_height(r.left)
	update_height(r)
```

### *****Balancing an AVL*****

Putting the definitions of `rotate_left()` and `rotate_right()` together, we can construct the pseudocode for balancing an AVL tree (restoring the core AVL property):

```python
balance(r):
	if bf(r) == -2:
		if bf(r.left) == 1:
			rotate_left(r.left)
		rotate_right(r)
	else if bf(r) == 2:
		if bf(r.right) == -1:
			rotate_right(r.right)
		rotate_left(r)
```

This assumes that `bf(r)` is defined as follows:

```python
bf(r):
	return r.left ? r.left : -1 - r.right ? r.right : -1
```

## Graphs

*Exam 3*

### **********Definitions**********

A given graph $G$ is composed of a set of vertices $V$ and  edges $E$ such that we say $G=(V,E)$.

A graph is directed if

$$
\exists(v,u)\space (v,u)\neq(u,v)
$$

By the numbers:

- If directed, max number of edges is $n(n-1)$
- If undirected, max number of edges is $\frac{n(n-1)}{2}$
- For both, $n$ vertices implies $O(n^2)$ edges
- If the graph has $\Theta(n^2)$ edges, it is *****dense*****
- If the graph has $O(n)$ edges, it is ****sparse****

### ********Representation********

Graphs can be represented by either an adjacency list or an adjacency matrix. For example, for the following graph:

![g.PNG](CSE%20247%20Notes%2014c56a9a63644ee6a5c8752f8b3aae2a/g.png)

We can create the following adjacency matrix:

$$
\begin{bmatrix}
     0 & 0 & 1 & 1 & 0\\ 
     0 & 0 & 0 & 0 & 1\\
     1 & 0 & 0 & 1 & 0\\
     1 & 0 & 1 & 0 & 0\\
     0 & 1 & 0 & 0 & 0\\   
\end{bmatrix}
$$

And the following adjacency list:

$$
1\Longrightarrow \{3, 4\}\\
2\Longrightarrow \{\space\space\space\space5\}\\
3\Longrightarrow \{1, 4\}\\
4\Longrightarrow \{1, 3\}\\
5\Longrightarrow \{\space\space\space\space2\}\\
$$

| Property | List | Matrix |
| --- | --- | --- |
| Space to represent | $\Theta(|V|+|E|)$ | $\Theta(|V|^2)$ |
| Time to check if edge $(u, v)$ exists | $\Theta(|E|)$ | $O(1)$ |
| Time to enumerate all edges in $G$ | $\Theta(|V|+|E|)$ | $\Theta(|V|^2)$ |

### *Breadth-first Search*

BFS uses a queue to traverse a graph by dequeuing vertices along connecting edges:

```python
while not Q.isEmpty():
	u = Q.dequeue()
	for (u, w) in u.edges:
		if not w.marked:
			w.marked = True
			Q.enqueue(w)
```

To augment BFS to extract useful information, such as the distance from the starting vertex to any given vertex on the graph as well as the path to said vertex via parent pointers as seen below, we can simply alter the body of the main loop:

```python
v.marked = True
v.distance = 0
v.parent = None
Q.enqueue(v)

while not Q.isEmpty():
	u = Q.dequeue()
	for (u, w) in u.edges:
		if not w.marked:
			w.marked = True
			w.distance = u.distance + 1    # new code
			w.parent = u                   # new code
			Q.enqueue(w)
```

Total cost of BFS is $\Theta(|V|+|E|)$.

### *Depth-first Search*

DFS finds all vertices reachable from a given $v$ before completing $v$:

```python
DFS(v):
	v.start = time++
	for (v, u) in edges:
		if not u.start:
			DFS(u)
	v.finish = time++
```

![Untitled](CSE%20247%20Notes%2014c56a9a63644ee6a5c8752f8b3aae2a/Untitled%201.png)

Total cost of DFS is $\Theta(|V|+|E|)$.

DFS has a number of applications, an example being the fact that if $DFS(v)$ is called on a vertex $v$ which has been started (visited) but not finished, then the graph necessarily contains a cycle.

### *Topological Order*

DFS can also be used to find the topological order, defined as follows:

$$
\small\text{A topological order on a directed, acyclic graph (DAG) is any total ordering of}\\ \text{the vertices consistent with the partial order defined by the edges.}
$$

## Dijkstra’s Algorithm

****Exam 3****

### *Definitions*

For a weighted graph, Dijkstra’s Algorithm finds the path of least total weight by keeping track of the total weight of the shortest path to a given vertex $v$, $W(v)$, and computing the weight of the shortest path to the next vertex $u$ using the following formula:

$$
W(u)=W(v)+w(v,u)\\
\small\text{only if}\\
\normalsize W(v)+w(u,v)<W(u)
$$

where $w(x,y)$ represents the weight of the edge between vertices $x$ and $y$.

A pseudocode definition of Dijkstra’s Algorithm with a priority queue $PQ$ would be:

```python
v.dist = 0
H[v] = PQ.insert(vertices[0])

for v in vertices:
	u.dist = inf
	H[u] = PQ.insert(u)

while not PQ.isEmpty():
	v = PQ.extractMin()
	for (v, u) in edges:
		if v.dist + w(v, u) < u.dist:
			u.dist = v.dist + w(u, v)
			H[u].updatePriority(u.dist)
```

## Minimum Spanning Trees (MSTs)

*Exam 3*

### *Definitions*

A minimum spanning tree is a graph in which all vertices are connected to one another, without creating cycles, with total path length minimized such that, for some set of edges $T$

$$
W(T)=\sum_{e \in T}w(e)
$$

is minimized. If a cycle does exist when a MST is constructed, there is always some edge which can be removed to decrease total path length without disconnecting the vertices. Since $T$ is an undirected, acyclic graph, it is a tree. 

![Untitled](CSE%20247%20Notes%2014c56a9a63644ee6a5c8752f8b3aae2a/Untitled%202.png)

### *Greedy Algorithm Construction*

To construct an MST properly from a set of vertices, we can use a ******greedy****** algorithm. A greedy algorithm is a heuristic which makes locally optimal choices at each stage. In the case of the construction of an MST we can use the following greedy principle for edge set $T$ on graph $G$:

- Define some criterion to apply when picking edges
- Pick the best edge via the criterion and add it to $T$
- Keep picking edges until $T$ spans $G$

### *Prim’s Algorithm*

For Prim’s Algorithm, the criterion for choosing an edge is as follows:

> Choose the edge with the minimum weight $w(e)$ that connects a vertex in $T$ to one not in $T$
> 

The only difference between Dijkstra’s Algorithm and Prim’s is the greedy criterion applied (Dijkstra’s utilizes the *total weight* of the graph $W(T)$ while Prim’s utilizes the weight of the last edge $w(e)$).

A pseudocode definition of Prim’s Algorithm with a priority queue $PQ$ would be:

```python
v.conn = 0
D[v] = PQ.insert(vertices[0])

for u in vertices:
	u.conn = inf
	D[u] = PQ.insert(u)

while not PQ.isEmpty():
	v = PQ.extractMin()
	for (v, u) in edges:
		if w(v, u) < u.conn:
			u.conn = w(u, v)
			D[u].updatePriority(u.conn)
```

Prim’s Algorithm runs in $\Theta(|E|+|V|\log|V|)$ time using a binary heap (same as Dijkstra’s).

### *Kruskal’s Algorithm*

Kruskal’s criterion is as follows:

> Add to $T$ the edge $e$ of minimum $w(e)$ that does not form a cycle with edges already in $T$
> 

Kruskal’s Algorithm runs in $\Theta(|E|\log|V|)$ time.